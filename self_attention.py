# -*- coding: utf-8 -*-
"""Self-Attention.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VrAIh5nS18VmnsE-xZI8UzVxLNz2eqaC
"""

import numpy as np

X = np.array([
    [1.0, 0.5, 0.2, 0.1],
    [0.9, 0.4, 0.3, 0.2],
    [0.8, 0.3, 0.4, 0.5]
])

seq_len, d_model = X.shape
d_k = d_model

W_Q = np.random.rand(d_model, d_k)
W_K = np.random.rand(d_model, d_k)
W_V = np.random.rand(d_model, d_k)

Q = X @ W_Q
K = X @ W_K
V = X @ W_V

print("Q:\n", Q)
print("K:\n", K)
print("V:\n", V)

scores = Q @ K.T

scaled_scores = scores / np.sqrt(d_k)

def softmax(x):
    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))
    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)

attention_weights = softmax(scaled_scores)

print("\nAttention Weights:\n", attention_weights)

output = attention_weights @ V

print("\nFinal Self-Attention Output:\n", output)

