# -*- coding: utf-8 -*-
"""Multi-Head_Attention.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vg336L_WujbE6OTEWB50vzzQayaEjAgc
"""

import numpy as np

X = np.array([
    [1.0, 0.5, 0.2, 0.1],
    [0.9, 0.4, 0.3, 0.2],
    [0.8, 0.3, 0.4, 0.5]
])

seq_len, d_model = X.shape
num_heads = 2
head_dim = d_model // num_heads

def softmax(x):
    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))
    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)

Wq = [np.random.rand(d_model, head_dim) for _ in range(num_heads)]
Wk = [np.random.rand(d_model, head_dim) for _ in range(num_heads)]
Wv = [np.random.rand(d_model, head_dim) for _ in range(num_heads)]

head_outputs = []

for h in range(num_heads):
    Q = X @ Wq[h]
    K = X @ Wk[h]
    V = X @ Wv[h]

    scores = Q @ K.T / np.sqrt(head_dim)
    attention_weights = softmax(scores)
    head_output = attention_weights @ V
    head_outputs.append(head_output)

multi_head_output = np.concatenate(head_outputs, axis=-1)

W0 = np.random.rand(num_heads * head_dim, d_model)  # (d_model, d_model)

multi_head_output = multi_head_output @ W0

print("Final Projected Multi-Head Attention Output:\n", multi_head_output)

