# -*- coding: utf-8 -*-
"""Word_Embedding_BERT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KmsXk4GoueabcxoTBOk545WnC22qR622
"""

import torch

from transformers import BertTokenizer, BertModel

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')

encoding = tokenizer(
    "The king is strong.",
    "The queen is wise.",
    padding=True,
    truncation=True,
    return_tensors="pt",
    return_token_type_ids=True,
    return_attention_mask=True
)

input_ids = encoding["input_ids"]
token_type_ids = encoding["token_type_ids"]
attention_mask = encoding["attention_mask"]

print("Input IDs:\n", input_ids)
print("Token Type IDs (Segment IDs):\n", token_type_ids)

word_embeddings = model.embeddings.word_embeddings(input_ids)
position_ids = torch.arange(0, input_ids.size(1), dtype=torch.long).unsqueeze(0)
position_embeddings = model.embeddings.position_embeddings(position_ids)
token_type_embeddings = model.embeddings.token_type_embeddings(token_type_ids)

final_embeddings = word_embeddings + position_embeddings + token_type_embeddings

print("\nWord Embeddings Shape:", word_embeddings.shape)
print("Position Embeddings Shape:", position_embeddings.shape)
print("Token Type Embeddings Shape:", token_type_embeddings.shape)
print("Final Embeddings Shape:", final_embeddings.shape)

outputs = model(inputs_embeds=final_embeddings, attention_mask=attention_mask)
last_hidden_states = outputs.last_hidden_state

print("\nLast hidden states shape:", last_hidden_states.shape)

outputs

